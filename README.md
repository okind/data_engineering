# data_engineering
Building a personal warehouse in SnowFlake, ETL wirh Airflow and dbt tool

Snowflake quick start guide:
Data Engineering with Apache Airflow, Snowflake & dbt
https://quickstarts.snowflake.com/guide/data_engineering_with_apache_airflow/index.html?index=..%2F..index#1

In this quick start lab I built a simple working pipeline from csv file to Snowflake using dbt for data transformation and Airflow for pipeline scheduling.
1. Created an account in Snowflake.
2. Created a warehouse in Snowflake.
3. Created a user, set user permissions.
4. Installed Docker Desktop.
5. Created Docker Image with Airflow and dbt.
6. Created 2 DAGs in Airflow.
7. First DAG is named 1_init_once_seed_data.
8. Second DAG is named 
9. 
